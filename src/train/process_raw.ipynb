{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = \"OHApps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def find_ets_files(directory):\n",
    "    ets_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.ets'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                ets_files.append({'file_path': full_path, 'content': content})\n",
    "    return ets_files\n",
    "\n",
    "def save_as_jsonl(data, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'ets_files.jsonl'\n",
    "ets_files = find_ets_files(repository)\n",
    "save_as_jsonl(ets_files, output_file)\n",
    "print(f\"Saved {len(ets_files)} .ets file paths and contents to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗.test.ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ets_files = []\n",
    "with open(\"ets_files.jsonl\", \"r\") as f:\n",
    "    ets_files = [json.loads(line) for line in f]\n",
    "    for file in ets_files:\n",
    "        ### 不要以.test.ets结尾的文件\n",
    "        if file['file_path'].endswith('.test.ets'):\n",
    "            continue\n",
    "        else:\n",
    "            valid_ets_files.append(file)\n",
    "\n",
    "save_as_jsonl(valid_ets_files, \"valid_ets_files.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗raw_data为空的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_ets_files.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    ### raw_data为空\n",
    "    for line in data:\n",
    "        d = json.loads(line)\n",
    "        if d['content'] == '':\n",
    "            ## 删除该行\n",
    "            data.remove(line)\n",
    "\n",
    "with open(\"valid_ets_files.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for line in data:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选出评测集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 选取前面10%的数据作为评测集\n",
    "with open(\"valid_ets_files.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    for i in range(int(len(data)/10)):\n",
    "        with open(\"test.jsonl\", 'a', encoding='utf-8') as f:\n",
    "            f.write(data[i])\n",
    "\n",
    "with open(\"train.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for i in range(int(len(data)/10), len(data)):\n",
    "        f.write(data[i])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 评测集的数据需要进行切分，即将raw_data的内容切分成perv和target两部分\n",
    "import re\n",
    "import random\n",
    "with open(\"test.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        d = json.loads(line)\n",
    "        raw_data = d['content']\n",
    "        ## 在某行的末尾随机进行切分，根据文件行数来切分吧，比如行数多的就多切几条数据\n",
    "        lines = raw_data.split('\\n')\n",
    "        n = len(lines) / 100\n",
    "\n",
    "        for i in range(int(n)):\n",
    "            ## 随机选择一行进行切分\n",
    "            idx = random.randint(8, len(lines)-1)\n",
    "            prev = '\\n'.join(lines[:idx])\n",
    "            target = '\\n'.join(lines[idx:])\n",
    "            with open(\"test_split.jsonl\", 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps({\n",
    "                    \"file_path\": d['file_path'],\n",
    "                    \"prev\": prev,\n",
    "                    \"target\": target\n",
    "                }, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ArkTSDataset(Dataset):\n",
    "    def __init__(self, input_jsonl_file, output_jsonl_file, tokenizer, max_length=1024, overlap=128):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        try:\n",
    "            with open(input_jsonl_file, 'r', encoding='utf-8') as infile, \\\n",
    "                 open(output_jsonl_file, 'w', encoding='utf-8') as outfile:\n",
    "                for line in infile:\n",
    "                    data = json.loads(line)\n",
    "                    content = data['content']\n",
    "                    file_path = data['file_path']\n",
    "                    chunks = self.process_content(content, file_path)\n",
    "                    \n",
    "                    for chunk in chunks:\n",
    "                        new_data = {\n",
    "                            'file_path': file_path,\n",
    "                            'content': self.tokenizer.decode(chunk)\n",
    "                        }\n",
    "                        json.dump(new_data, outfile)\n",
    "                        outfile.write('\\n')\n",
    "                    \n",
    "                    self.examples.extend(chunks)\n",
    "            print(f\"Processed data saved to {output_jsonl_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "\n",
    "    def process_content(self, content, file_path):\n",
    "        # 添加特殊注释来标记 <cangjie> 代码\n",
    "        content_with_markers = f\"// BEGIN <arkts>\\n{content}\\n// END <arkts>\"\n",
    "        tokens = self.tokenizer.encode(content_with_markers, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(tokens), self.max_length - self.overlap):\n",
    "            chunk = tokens[i:i + self.max_length - 2]  # -2 为了添加特殊标记\n",
    "            chunk = [self.tokenizer.bos_token_id] + chunk + [self.tokenizer.eos_token_id]\n",
    "            \n",
    "            if len(chunk) < self.max_length:\n",
    "                chunk = chunk + [self.tokenizer.pad_token_id] * (self.max_length - len(chunk))\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": torch.tensor(self.examples[idx])}\n",
    "\n",
    "# 设置和使用\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:15777\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:15777\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", cache_dir=\"./model\", trust_remote_code=True)\n",
    "\n",
    "# 创建数据集，同时处理并保存新的jsonl文件\n",
    "input_file = 'valid_ets_files.jsonl'\n",
    "output_file = 'processed_ets_files.jsonl'\n",
    "dataset = ArkTSDataset(input_file, output_file, tokenizer)\n",
    "print(f\"Total number of chunks: {len(dataset)}\")\n",
    "\n",
    "# 如果您想查看处理后的数据，可以这样做：\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:  # 只打印前5行作为示例\n",
    "            print(json.loads(line))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ArkTSDataset(Dataset):\n",
    "    def __init__(self, input_jsonl_file, output_jsonl_file, tokenizer, max_length=1024, overlap=128):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        with open(input_jsonl_file, 'r', encoding='utf-8') as infile, \\\n",
    "             open(output_jsonl_file, 'w', encoding='utf-8') as outfile:\n",
    "            for line in infile:\n",
    "                data = json.loads(line)\n",
    "                content = data['content']\n",
    "                file_path = data['file_path']\n",
    "                chunks = self.process_content(content, file_path)\n",
    "                \n",
    "                # 保存处理后的chunks到新的jsonl文件\n",
    "                for chunk in chunks:\n",
    "                    new_data = {\n",
    "                        'file_path': file_path,\n",
    "                        'content': chunk\n",
    "                    }\n",
    "                    json.dump(new_data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "                self.examples.extend(chunks)\n",
    "        print(f\"Processed data saved to {output_jsonl_file}\")\n",
    "\n",
    "    def process_content(self, content, file_path):\n",
    "        header = f\"<｜arkts｜>\\nFile: {file_path}\\n\"\n",
    "        content_with_header = header + content\n",
    "        \n",
    "        tokens = self.tokenizer.encode(content_with_header, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(tokens), self.max_length - self.overlap):\n",
    "            chunk = tokens[i:i + self.max_length - 2]\n",
    "            \n",
    "            # 如果不是第一个chunk，添加header\n",
    "            if i != 0:\n",
    "                header_tokens = self.tokenizer.encode(header, add_special_tokens=False)\n",
    "                chunk = header_tokens + chunk[-(self.max_length - 2 - len(header_tokens)):]\n",
    "            \n",
    "            chunk_text = self.tokenizer.decode(chunk)\n",
    "            chunk_text = f\"<｜chunk_start｜>{chunk_text}<｜chunk_end｜>\"\n",
    "            chunks.append(chunk_text)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"content\": self.examples[idx]}\n",
    "\n",
    "# 设置和使用\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-Coder-V2-Lite-Base')\n",
    "new_tokens = [\"<｜arkts｜>\", \"<｜chunk_start｜>\", \"<｜chunk_end｜>\"]\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n",
    "\n",
    "# 创建数据集，同时处理并保存新的jsonl文件\n",
    "input_file = 'valid_ets_files.jsonl'\n",
    "output_file = 'processed_ets_train_files.jsonl'\n",
    "dataset = ArkTSDataset(input_file, output_file, tokenizer)\n",
    "print(f\"Total number of chunks: {len(dataset)}\")\n",
    "\n",
    "# 如果您想查看处理后的数据，可以这样做：\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:  # 只打印前5行作为示例\n",
    "            print(json.loads(line))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    print(checkpoint['optimizer_state_dict'])\n",
    "    print(checkpoint['model_state_dict'])\n",
    "\n",
    "checkpoint_path = '/home/CangjieLLM/src/code/train/checkpoints/temp_checkpoint.pt'\n",
    "load_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'processed_ets_files.jsonl'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    for i, line in enumerate(data):\n",
    "        if i == 7:\n",
    "            d = json.loads(line)\n",
    "            print(d['content'])\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cangjieLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
